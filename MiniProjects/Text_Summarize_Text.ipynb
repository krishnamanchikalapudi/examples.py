{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea3a5a71-59fe-482a-915a-4022d97db566",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8676d671-7795-4b02-a152-a58f4997a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# BEGIN: fix Python or Notebook SSL CERTIFICATE_VERIFY_FAILED\n",
    "import os, ssl\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None)):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# END: fix Python or Notebook SSL CERTIFICATE_VERIFY_FAILED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7a0f67-2d31-4c8c-9036-de9b0b1c090b",
   "metadata": {},
   "source": [
    "## Installing pre-requsite libraries\n",
    "* https://pypi.org/project/bert-extractive-summarizer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a736be5-417b-41dc-b36d-674622a54f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ornado (/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ornado (/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages)\u001b[0m\n",
      "Requirement already satisfied: sumy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.8.1)\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (4.8.1)\n",
      "Requirement already satisfied: sentencepiece in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.1.96)\n",
      "Requirement already satisfied: requests>=2.7.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from sumy) (2.25.1)\n",
      "Requirement already satisfied: nltk>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from sumy) (3.6.2)\n",
      "Requirement already satisfied: breadability>=0.1.20 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from sumy) (0.1.20)\n",
      "Requirement already satisfied: pycountry>=18.2.23 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from sumy) (20.7.3)\n",
      "Requirement already satisfied: docopt<0.7,>=0.6.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from sumy) (0.6.2)\n",
      "Requirement already satisfied: lxml>=2.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from breadability>=0.1.20->sumy) (4.6.3)\n",
      "Requirement already satisfied: chardet in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from breadability>=0.1.20->sumy) (4.0.0)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk>=3.0.2->sumy) (4.61.1)\n",
      "Requirement already satisfied: regex in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk>=3.0.2->sumy) (2021.4.4)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk>=3.0.2->sumy) (7.1.2)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk>=3.0.2->sumy) (1.0.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests>=2.7.0->sumy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests>=2.7.0->sumy) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests>=2.7.0->sumy) (1.26.5)\n",
      "Requirement already satisfied: pyyaml in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: sacremoses in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ornado (/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ornado (/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ornado (/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sumy transformers sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c7e282-f10e-4189-bd45-ed66aad4c174",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fb50876-0c4c-4086-ad1c-c577f44f98f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fbbd8a3-56be-4a2d-862c-f495785fae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"Text_Summarize_Text/content.txt\"\n",
    "\n",
    "output_sentences_count = 10\n",
    "\n",
    "with open(content, \"r\", encoding=\"utf-8\") as f: # open(r'C:\\Users\\...site_1.html', \"r\") as f:\n",
    "    article = f.read()  \n",
    "    \n",
    "# article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0407568-6e9d-4926-afd3-fcff0127bb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This leads to situations in which “people consent to the collection, use, and disclosure of their personal data when it is not in their self-interest to do so” (Solove 2013, p. 1895).\n",
      "Not only this: the long-term storage of big data means that situations may arise in which there is a desire to use it for purposes that may not be remotely connected to those for which it was gathered (and for which consent was given) in the first place.\n",
      "What alternative measures might be taken to ensure that genuinely informed consent is obtained from those who give up their data for all the purposes to which it might be subsequently put?\n",
      "If big data might be used in such a way as to turn a profit, then what regulation should exist around its use, in terms both of its commercial exploitation, and of trade in the data itself?\n",
      "It has yet to be properly held to account by either ethicists or legislators, and it would appear that the longer this situation continues, the more this business is likely to mushroom, and the harder it is likely to become to impose ethical or legal restrictions upon it.\n",
      "Generation and Analysis of Data A raft of issues exists around the gathering of data, its treatment and analysis, and the presentation of the results of such analysis by experts to those who have little understanding of the way those results have been obtained, but who may be making potentially far-reaching decisions based on them (recall that one task of the data scientist is to be a storyteller: that is, to formulate persuasive narratives accounting for the results of data analyses).\n",
      "This will frequently involve the production of visualisations of the data and of what it is telling us, through graphs, charts, diagrams, and so on; and these visualisations may, themselves, incorporate conscious or unconscious bias in those who have prepared them, which might encourage those to whom the data is being presented to read it in particular ways.\n",
      "Such practical actions as these are likely to come from the organisational leadership of religious organisations like churches, but such bodies have further roles to play in that they are communities which allow issues to be discussed and “owned” by a broader public than that which is likely to engage with issues when they are presented in purely ethical terms.\n",
      "A further complication is the speed with which data science is advancing, which means that (for example) the application of legal and ethical restrictions to the practice of that science will always risk being several steps behind the point that it has currently reached.\n",
      "Religious communities may offer critiques based on particular sets of values which treat human beings as more than simply data points, and they may offer fora for discussions which may aid in the dissemination of information about data science, as well as an opportunity to critique it.\n"
     ]
    }
   ],
   "source": [
    "my_parser = PlaintextParser.from_string(article, Tokenizer('english'))\n",
    "\n",
    "# Creating a summary of 3 sentences.\n",
    "lex_rank_summarizer = LexRankSummarizer()\n",
    "lexrank_summary = lex_rank_summarizer(my_parser.document, sentences_count = output_sentences_count)\n",
    "\n",
    "# Printing the summary\n",
    "for sentence in lexrank_summary:\n",
    "  print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e3412b-a7ec-4cf8-98cf-730ac31d38e3",
   "metadata": {},
   "source": [
    "## LSA (Latent semantic analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f59ae264-120f-426e-8359-2eab5419f6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It further suggests that insights from the religious domain might be of considerable value in developing these new approaches to big data.\n",
      "Extremely large datasets are not simply quantitatively different to smaller ones: their sheer scale brings about a “step change”, making them qualitatively distinct, too.\n",
      "In practice, when data has been anonymized (or “de-identified”) it may well be possible to break that anonymity by triangulating between multiple datasets.\n",
      "To illustrate this, consider the data which is routinely stored whenever individuals give samples as part of a medical procedure.\n",
      "Guidelines and recommendations are a good start, but more is surely required to ensure that all data science practitioners act in ethically responsible ways.\n",
      "Now, although a science based on the manipulation of data might appear “objective” to an outsider, we have seen that this is, in fact, very far from being the case, and that data scientists are required to use considerable personal skill and judgment in their work.\n",
      "(British Medical Association 2012, p. 887)), and it has been suggested that data scientists, too, might undertake a similar oath, holding them to particular ethical standards in the practice of their craft (O’Neil 2016, pp.\n",
      "Christianity, in common with many other faith traditions, places a high value on notions of accountability and fairness.\n",
      "A dialogue between data scientists and theologians concerning hermeneutical practices could be an important way in which skills developed in the service of a religious tradition might also valuably inform practices within this developing new science—an idea I have developed more fully elsewhere (Fuller 2015, pp.\n",
      "The arrival of big data has brought with it concerns which are only starting to be appreciated and discussed.\n"
     ]
    }
   ],
   "source": [
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "# creating the summarizer\n",
    "lsa_summarizer = LsaSummarizer()\n",
    "lsa_summary = lsa_summarizer(my_parser.document, sentences_count = output_sentences_count)\n",
    "\n",
    "# Printing the summary\n",
    "for sentence in lsa_summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfb3e13-6b2d-4a1f-92ed-6e6e3d0ed0d3",
   "metadata": {},
   "source": [
    "## Luhn Summarization algorithm’s approach is based on TF-IDF (Term Frequency-Inverse Document Frequency). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5cda9cd-3069-4e47-8183-184bb31689d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In practice, however, it has been observed that this is a process which is geared more towards limiting the liabilities of those harvesting the data rather than genuinely informing the data subjects: “the parties gathering the data typically attempt to minimize the ability of the person about whom the data is being gathered to comprehend the scope of data, and its usage, through a mixture of sharp design and obscure legal jargon” (Wilbanks 2014, p. 235).\n",
      "The practical difficulties of managing privacy and generating informed consent have been summed up as: (1) people do not read privacy policies; (2) if people read them, they do not understand them; (3) if people read and understand them, they often lack enough background knowledge to make an informed choice; and (4) if people read them, understand them, and can make an informed choice, their choice may be skewed by various decision-making difficulties.\n",
      "However, given that not every disaster may be foreseen, and that those with nefarious purposes will always be seeking new ways in which to circumvent security measures, it is equally important to have robust systems in place to detect such attacks when they are made, respond appropriately to them, and ensure that the system which has been attacked is restored to its pre-attack state—with, if necessary, appropriate new safeguards in place.\n",
      "Generation and Analysis of Data A raft of issues exists around the gathering of data, its treatment and analysis, and the presentation of the results of such analysis by experts to those who have little understanding of the way those results have been obtained, but who may be making potentially far-reaching decisions based on them (recall that one task of the data scientist is to be a storyteller: that is, to formulate persuasive narratives accounting for the results of data analyses).\n",
      "This will frequently involve the production of visualisations of the data and of what it is telling us, through graphs, charts, diagrams, and so on; and these visualisations may, themselves, incorporate conscious or unconscious bias in those who have prepared them, which might encourage those to whom the data is being presented to read it in particular ways.\n",
      "I suggest that there are, broadly, four ways in which this might happen, and I would urge that at least some of these ways of addressing the complex issues raised by big data can be informed by thinking, and by practical activity, which may come from theological and religious communities.\n",
      "Much of the discussion around the regulation of big data thus far has focussed on the U.S. context, but it should be noted that other, well-developed legal frameworks are operative elsewhere, such as those set out in the UK’s Data Protection Act 1998 and the European Union’s General Data Protection Regulation of 2016 (see, e.g., (Elias 2014)).\n",
      "Where such values are seen to be being flouted—for example, through keeping people ignorant of the consequences of their giving up data, or through the use of big data to reinforce social inequalities, or through the gathering, storage, and trade of personal data without effective consent, by unaccountable commercial organisations—the churches might speak out against such practices, and lobby for their restriction.\n",
      "Such practical actions as these are likely to come from the organisational leadership of religious organisations like churches, but such bodies have further roles to play in that they are communities which allow issues to be discussed and “owned” by a broader public than that which is likely to engage with issues when they are presented in purely ethical terms.\n",
      "As noted above, data scientists are engaged in a complex interpretative exercise which involves recognition of the history and context of the data which they are analysing, the biases contained both in it and in the analytical techniques which are being used to explore it, their own inbuilt biases, both conscious and unconscious, and the complexities of the (quite possibly, commercial) context in which their work is being carried out.\n"
     ]
    }
   ],
   "source": [
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "\n",
    "#  Creating the summarizer\n",
    "luhn_summarizer = LuhnSummarizer()\n",
    "luhn_summary = luhn_summarizer(my_parser.document, sentences_count = output_sentences_count)\n",
    "\n",
    "# Printing the summary\n",
    "for sentence in luhn_summary:\n",
    "  print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c561e7-3eb2-4d21-8440-388a74163b22",
   "metadata": {},
   "source": [
    "## extractive method is the KL-Sum algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61b69158-c912-411f-856f-89a75241de7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All of these concerns are ongoing.\n",
      "The distinctiveness of big data goes beyond straightforward issues of size.\n",
      "3.\n",
      "What concerns are specific to the big data context?\n",
      "We may immediately note that anyone who engages with services which make use of computers surrenders data to those who run those computers—whether they are consciously aware of it or not.\n",
      "Ownership Are data property?\n",
      "Should it be the person to whom it relates, or the organisation which has gathered it?\n",
      "Should the data be freely available, to assist the researchers?\n",
      "Attention needs to be paid not only to the analysis of data, but to the presentation of the fruits of that analysis.\n",
      "The End of Science?\n"
     ]
    }
   ],
   "source": [
    "from sumy.summarizers.kl import KLSummarizer\n",
    "kl_summarizer = KLSummarizer()\n",
    "kl_summary = kl_summarizer(my_parser.document, sentences_count = output_sentences_count)\n",
    "\n",
    "# Printing the summary\n",
    "for sentence in kl_summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0df982-ff6a-4d3b-91f9-df72d95032ad",
   "metadata": {},
   "source": [
    "## Summarization with T5 Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f706df3-9cee-4d40-938a-6a5fb8ea88cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> <extra_id_0>,<extra_id_1>, and<extra_id_2>,<extra_id_3>, and<extra_id_4>,<extra_id_5>, and their sheer scale brings\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
    "\n",
    "my_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "input_ids = tokenizer.encode(article, return_tensors='pt', max_length=750)\n",
    "summary_ids = my_model.generate(input_ids)\n",
    "\n",
    "t5_summary = tokenizer.decode(summary_ids[0])\n",
    "print(t5_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1981d953-3c2d-48c9-a2da-25337f0a642e",
   "metadata": {},
   "source": [
    "# GPT-2 Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a7366dc-706d-42b9-bcb5-28d96543ad0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 750, but ``max_length`` is set to 20.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As any technology develops it might be expected that its increasing capabilities give rise to a succession of ethical issues, and computer technology is certainly no exception to this (Tavani 2013a, pp. 6 ff.). Concerns which have been raised in the past include the deskilling of the workforce, increased unemployment, and the health, stress, and isolation of workers, together with issues relating to the storage of personal data in the form of databases (Barbour 1992, pp. 146 ff.). A further concern is the implication of computers in broadening divisions between rich and poor, through the opening up of imbalances between those who have access to computer facilities and the benefits which they bring, and those who do not (Tavani 2013a, p. 305; Barbour 1992, p. 156). All of these concerns are ongoing.\n",
      "\n",
      "In recent years the increasingly widespread use of computers in all walks of life, from the PCs and smartphones that many consumers use on a daily basis to the supercomputers used in research programmes in astronomy, physics, and medicine, has generated the phenomenon that has become known as “big data”:1 extremely large, often highly heterogeneous, datasets that require novel techniques and new sets of skills to interrogate them. In turn, this has led to a new set of ethical issues surrounding big data. The aim of this paper is to identify, describe, and evaluate some of these ethical issues, and to suggest ways in which some of them may be addressed. It further suggests that insights from the religious domain might be of considerable value in developing these new approaches to big data.\n",
      "\n",
      "Before going further, it is worth exploring exactly what big data is understood to mean. Although the term is widely used, there is little agreement around a definition, in part because what counts as “big” in this context is changing so rapidly. The description by Laney (2001) in terms of the “three Vs” (volume, variety, and velocity) has been widely quoted: on this understanding, big data is characterised as being concerned with very large quantities of data, which is highly heterogeneous, and which is generated at enormous speed. Kitchin (2014, p. 68) comments that, in addition to this, big data is exhaustive in scope, fine-grained in resolution, relational in nature (enabling different datasets to be linked), and both flexible and scalable (enabling new fields to be added to it, and the rapid extension of the size of the dataset). Other, broader, definitions have been offered (e.g., (boyd and Crawford 2012, p. 663)).\n",
      "The distinctiveness of big data goes beyond straightforward issues of size. Mayer-Schönberger and Cukier point out that with the accumulation of so much data, “something new and special is taking place. Not only is the world awash with more information than ever before, but that information is growing faster. The change of scale has led to a change of state. The quantitative change has led to a qualitative one” ((Mayer-Schönberger and Cukier 2013, p. 6), my emphasis). Similarly, Kitchin notes that “It is becoming clear that big data have a number of inherent characteristics that make them qualitatively different to previous forms of data” (Kitchin 2014, p. 79). Extremely large datasets are not simply quantitatively different to smaller ones: their sheer scale brings about a “step change”, making them qualitatively distinct, too. This means that different tools and different models are required for their handling and analysis.\n",
      "This, in turn,\n"
     ]
    }
   ],
   "source": [
    "# Importing model and tokenizer\n",
    "from transformers import GPT2Tokenizer,GPT2LMHeadModel\n",
    "\n",
    "# Instantiating the model and tokenizer with gpt-2\n",
    "tokenizer=GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model=GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Encoding text to get input ids & pass them to model.generate()\n",
    "inputs=tokenizer.batch_encode_plus([article], return_tensors='pt', max_length=750)\n",
    "summary_ids=model.generate(inputs['input_ids'], early_stopping=True)\n",
    "\n",
    "GPT_summary=tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(GPT_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4942e2ae-7ad4-4e58-a0a1-4fc585f85d89",
   "metadata": {},
   "source": [
    "# XLM Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8d9362d-8f7d-43c2-832f-f48481deee69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMWithLMHeadModel were not initialized from the model checkpoint at xlm-mlm-en-2048 and are newly initialized: ['transformer.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'XLMTokenizer' object has no attribute 'batch_encode_articles'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-aff72289a35a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Encoding text to get input ids & pass them to model.generate()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_encode_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0msummary_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'XLMTokenizer' object has no attribute 'batch_encode_articles'"
     ]
    }
   ],
   "source": [
    "# Importing model and tokenizer\n",
    "from transformers import XLMWithLMHeadModel, XLMTokenizer\n",
    "\n",
    "# Instantiating the model and tokenizer \n",
    "tokenizer = XLMTokenizer.from_pretrained('xlm-mlm-en-2048')\n",
    "model = XLMWithLMHeadModel.from_pretrained('xlm-mlm-en-2048')\n",
    "\n",
    "# Encoding text to get input ids & pass them to model.generate()\n",
    "inputs = tokenizer.batch_encode_articles([article], return_tensors='pt', max_length=512)\n",
    "summary_ids = model.generate(inputs['input_ids'], early_stopping=True)\n",
    "\n",
    "# Decode and print the summary\n",
    "XLM_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(XLM_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4a224a-a489-4f3e-a999-ea91033562b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
